{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf801531",
   "metadata": {},
   "source": [
    "# Step 1 - Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000b17ff",
   "metadata": {},
   "source": [
    "Import the required libraries here. You could use additional libraries to help with your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f60686cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import copy\n",
    "import math\n",
    "import random\n",
    "import string\n",
    "import pathlib\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from cs589.assignment1.utils.common import save_pickle_file, load_pickle_file, load_text_file\n",
    "\n",
    "base_path = pathlib.Path(\"cs589/assignment1/dataset/\")\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "856ce140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text):\n",
    "    return text.split()\n",
    "\n",
    "\n",
    "def load_qids(lang=\"java\"):\n",
    "    return [qid.strip(string.whitespace) for qid in load_text_file(base_path / pathlib.Path(f\"{lang}/{lang}_test_qid.txt\"))]\n",
    "\n",
    "\n",
    "def load_qid_dataframe(lang=\"java\"):\n",
    "    qid_dataframe = pd.read_csv(base_path / pathlib.Path(f\"{lang}/{lang}_cosidf.txt\"), \n",
    "                                sep=\"\\t\", \n",
    "                                usecols=[\"qid1\", \"qid2\", \"label\"],\n",
    "                                dtype={\"qid1\": str, \"qid2\": str, \"label\": int})\n",
    "    return qid_dataframe\n",
    "\n",
    "\n",
    "def load_corpus(lang=\"java\", verbose=False):\n",
    "    lines = load_text_file(base_path / pathlib.Path(f\"{lang}/{lang}_qid2all.txt\"))\n",
    "\n",
    "    record_list = list()\n",
    "    for line in tqdm(lines, disable=not verbose):\n",
    "        record_list.append(\n",
    "            {name: text.strip(string.whitespace) for name, text in zip([\"qid\", \"title\", \"question\", \"answer\"], line.split(\"\\t\"))}\n",
    "        )\n",
    "            \n",
    "    corpus_dataframe = pd.DataFrame(record_list)\n",
    "\n",
    "    return corpus_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4f3ae6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 159263/159263 [00:00<00:00, 321690.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        qid                                     title  \\\n",
      "0  31424546   eclipse mars starts exit code using jdk   \n",
      "1  31457289  efficient method updating observablelist   \n",
      "2  16777228                          set title jtable   \n",
      "3  27262998                  multiple websockets java   \n",
      "4  46137348      find runtime error nzec java program   \n",
      "\n",
      "                                            question  \\\n",
      "0  plan moving eclipse mars recently installed bi...   \n",
      "1  setup mysql database data makeshift server bui...   \n",
      "2  newbie java wanted set table header jtable tak...   \n",
      "3  deprecated ok opening connection specific port...   \n",
      "4  find runtime error nzec java program program r...   \n",
      "\n",
      "                                              answer  \n",
      "0  jdk bit download windows x version point vm mi...  \n",
      "1  need would work keep list instance serverlist ...  \n",
      "2  define variable containing column names must i...  \n",
      "3  trying achieve multiple function listen server...  \n",
      "4  try test code input like probably shall realiz...  \n"
     ]
    }
   ],
   "source": [
    "# take a look at the corpus\n",
    "pd.set_option(\"display.max_columns\", 10)\n",
    "\n",
    "java_corpus_dataframe = load_corpus(lang=\"java\", verbose=True)\n",
    "print(java_corpus_dataframe.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fff0b6",
   "metadata": {},
   "source": [
    "# Step 2 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e11f32",
   "metadata": {},
   "source": [
    "The following cell computes the term frequency (TF) for each word in each component in each StackOverflow question (indexed by the question ID qid)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d06223b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus_tf_dict(corpus_dataframe):\n",
    "    \"\"\" Input: corpus_dataframe, e.g.,    \n",
    "    \n",
    "         qid         title                 question          answer\n",
    " 0  31424546   eclipse mars   eclipse moving eclipse    jdk download   \n",
    "                                            \n",
    "        Output: corpus_tf_dict, the term frequency for each word in each component of each question, e.g., \n",
    "        {'31424546': {'title': {'eclipse': 1, 'mars': 1},\n",
    "                      'question': {'moving': 1, 'eclipse': 2},\n",
    "                      'answer': {'jdk': 1, 'download': 1}}}\n",
    "    \"\"\"\n",
    "    cnt_dataframe = copy.deepcopy(corpus_dataframe)\n",
    "    for c in [\"title\", \"question\", \"answer\"]:\n",
    "        cnt_dataframe[c] = cnt_dataframe[c].progress_apply(lambda x: Counter(split_text(x)))\n",
    "\n",
    "    corpus_tf_dict = cnt_dataframe.set_index(\"qid\").to_dict(\"index\")\n",
    "    \n",
    "    return corpus_tf_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e95f1c",
   "metadata": {},
   "source": [
    "The following cell computes the document length (dl) of each component in each StackOverflow question (indexed by the question ID qid)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2ffe4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus_dl_dict(corpus_dataframe):\n",
    "    \"\"\" Input: corpus_dataframe, e.g.,    \n",
    "         qid         title                 question          answer\n",
    "0  31424546   eclipse mars   eclipse moving eclipse    jdk download  \n",
    "\n",
    "        Output: corpus_dl_dict, the document length for each component from each question, e.g., \n",
    "        {'31424546': {'title': 2,\n",
    "                      'question': 3,\n",
    "                      'answer': 2}}\n",
    "    \"\"\"\n",
    "    length_dataframe = copy.deepcopy(corpus_dataframe)\n",
    "    for c in [\"title\", \"question\", \"answer\"]:\n",
    "        length_dataframe[c] = length_dataframe[c].progress_apply(lambda x: len(split_text(x)))\n",
    "\n",
    "    corpus_dl_dict = length_dataframe.set_index(\"qid\").to_dict(\"index\")\n",
    "    \n",
    "    return corpus_dl_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b54c579",
   "metadata": {},
   "source": [
    "The following cell computes the document frequency (DF) of each word in each StackOverflow question (indexed by the question ID qid). The definition of document frequency is how many document a word appears in, not to be confused with the word's frequency in the entire corpus. For example, the df of \"eclipse\" below is 2 instead of 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d892a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus_df_dict(corpus_dataframe):\n",
    "    \"\"\" Input: corpus_dataframe, e.g.,    \n",
    "         qid          title                 question          answer\n",
    " 0  31424546   eclipse mars   eclipse moving eclipse    jdk download  \n",
    "\n",
    "        Output: corpus_df_dict, the document length for each component from each question, e.g., \n",
    "        {'eclipse': 2, \"mars\": 1, \"moving\": 1, \"jdk\": 1, \"download\": 1}\n",
    "    \"\"\"\n",
    "    vectorizer = CountVectorizer(binary=True)\n",
    "\n",
    "    X = vectorizer.fit_transform(corpus_dataframe.title.tolist() + \\\n",
    "                                 corpus_dataframe.question.tolist() + \\\n",
    "                                 corpus_dataframe.answer.tolist())\n",
    "    corpus_df_dict = {token: doc_freq for token, doc_freq in \\\n",
    "                      zip(vectorizer.get_feature_names(), np.ravel(X.sum(axis=0)))}\n",
    " \n",
    "    return corpus_df_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87ad230",
   "metadata": {},
   "source": [
    "## Saving the Data Preprocessing Result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48b8e4e",
   "metadata": {},
   "source": [
    "After computing the TF, DF and dl, cache each of them in a pickle file to be loaded later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b43645be",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_path = pathlib.Path(\"pkl/\")\n",
    "if not pkl_path.exists(): pkl_path.mkdir()\n",
    "\n",
    "def save_preprocessing_results(lang):\n",
    "    print(f\"Processing {lang}...\")\n",
    "        \n",
    "    lang_pkl_path = pkl_path / lang\n",
    "    if not lang_pkl_path.exists(): os.mkdir(lang_pkl_path)\n",
    "\n",
    "    # load corpus and convert corpus to various required data\n",
    "    corpus_dataframe = load_corpus(lang=lang, verbose=True)\n",
    "\n",
    "    # obtain the dictionary for the term frequency for each word in each component of each question\n",
    "    corpus_tf_dict = get_corpus_tf_dict(corpus_dataframe)\n",
    "\n",
    "    # saving the term frequency dictionary\n",
    "    save_pickle_file(corpus_tf_dict, f\"pkl/{lang}/corpus_tf_dict.pkl\")\n",
    "\n",
    "    # obtain the dictionary for the document length for each component in each question \n",
    "    corpus_dl_dict = get_corpus_dl_dict(corpus_dataframe)\n",
    "    \n",
    "    # save the document length dictionary\n",
    "    save_pickle_file(corpus_dl_dict, f\"pkl/{lang}/corpus_dl_dict.pkl\")\n",
    "\n",
    "    # obtain the dictionary for the document frequency for each word in the corpus\n",
    "    corpus_df_dict = get_corpus_df_dict(corpus_dataframe)\n",
    "\n",
    "    # remove rare words\n",
    "    corpus_df_dict = {k: v for k, v in corpus_df_dict.items() if v >= 20}\n",
    "\n",
    "    # save the document frequency dictionary\n",
    "    save_pickle_file(corpus_df_dict, f\"pkl/{lang}/corpus_df_dict.pkl\")\n",
    "\n",
    "    return corpus_tf_dict, corpus_dl_dict, corpus_df_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde8b51b",
   "metadata": {},
   "source": [
    "Run the data processing pipeline for the 3 languages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35c3a94d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing python...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 128500/128500 [00:00<00:00, 365270.08it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 128500/128500 [00:00<00:00, 222908.43it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 128500/128500 [00:01<00:00, 73901.05it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 128500/128500 [00:02<00:00, 63383.70it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 128500/128500 [00:00<00:00, 450684.02it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 128500/128500 [00:00<00:00, 190371.39it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 128500/128500 [00:00<00:00, 181987.40it/s]\n",
      "c:\\users\\alexr\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing java...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 159263/159263 [00:00<00:00, 283188.72it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 159263/159263 [00:00<00:00, 216299.54it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 159263/159263 [00:02<00:00, 61040.11it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 159263/159263 [00:03<00:00, 51873.43it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 159263/159263 [00:00<00:00, 508015.29it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 159263/159263 [00:00<00:00, 200748.07it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 159263/159263 [00:00<00:00, 173633.46it/s]\n",
      "c:\\users\\alexr\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing javascript...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 174015/174015 [00:00<00:00, 292241.78it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 174015/174015 [00:00<00:00, 191971.86it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 174015/174015 [00:02<00:00, 70274.49it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 174015/174015 [00:03<00:00, 57171.79it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 174015/174015 [00:00<00:00, 503102.77it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 174015/174015 [00:00<00:00, 197627.73it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 174015/174015 [00:00<00:00, 180327.87it/s]\n",
      "c:\\users\\alexr\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "for lang in [\"python\", \"java\", \"javascript\"]:\n",
    "     save_preprocessing_results(lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651ada53",
   "metadata": {},
   "source": [
    "Create the folder result to store results for Question 4 - 6 if it does not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "030c0a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path = pathlib.Path(\"result\")\n",
    "if not result_path.exists(): result_path.mkdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d222bf",
   "metadata": {},
   "source": [
    "# Step 3 - Implement the TF-IDF and BM25 Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff144dfa",
   "metadata": {},
   "source": [
    "## Question 1 (30 pts)\n",
    "\n",
    "Compute the cosine similarity given dictionaries of word count, query_dict and candidate_dict. When working with term frequencies, they are extracted from corpus_tf_dict using corpus_tf_dict[qid][component] syntax)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21b5b3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine_similarity(query_tf_dict, \n",
    "                              candidate_tf_dict):\n",
    "    \"\"\" Input: query_tf_dict: a dict of word and its term frequency in query document, e.g.\n",
    "               {\"i\": 1, \"love\": 1, \"python\": 1}\n",
    "               candidate_tf_dict: a dict of word and its term frequency in the candidate document, e.g.\n",
    "               {\"i\": 1, \"like\": 1, \"c++\": 1}\n",
    "        Output: score: cosine similary between query and candidate documents\n",
    "                0.33333333333333337\n",
    "                \n",
    "    \"\"\"\n",
    "\n",
    "    score = 0\n",
    "    #############################################START HERE#############################################\n",
    "    # Question 1 (30 pts)\n",
    "    \n",
    "    # Converting dictionary values into numpy arrays\n",
    "    # according to dictionary keys i.e. terms\n",
    "    query_tf_dict_values = list(query_tf_dict.keys())\n",
    "    arr1 = np.array(query_tf_dict_values)\n",
    "    \n",
    "    candidate_tf_dict_values = list(candidate_tf_dict.keys())\n",
    "    arr2 = np.array(candidate_tf_dict_values)\n",
    "    \n",
    "    # Count the number of occurances\n",
    "    c1 = Counter(arr1)\n",
    "    c2 = Counter(arr2)\n",
    "    \n",
    "    # all items\n",
    "    all_items = set(c1.keys()).union(set(c2.keys()))\n",
    "    \n",
    "    x = [c1[k] for k in all_items]\n",
    "    y = [c2[k] for k in all_items]\n",
    "    \n",
    "    # calculate cosine similarity\n",
    "    # dot product of the two arrays divided by\n",
    "    # the multiplication of the norm of individual arrays\n",
    "    score = np.dot(x, y)/(np.linalg.norm(x)*np.linalg.norm(y))\n",
    "    \n",
    "    # the score calculated gives a runtime warning of\n",
    "    # invalid value encountered in double_scalars\n",
    "    \n",
    "    # this happens because there are mathematical operations that\n",
    "    # involve small or large multiplication\n",
    "    # however, this doesn't affect the output\n",
    "    # kindly check the output Q4.txt\n",
    "\n",
    "\n",
    "    ##############################################END HERE##############################################\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da556883",
   "metadata": {},
   "source": [
    "Test your compute_cosine_similarity implementation on the Python corpus when retrieving candidate's title using query's title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f6782b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexr\\AppData\\Local\\Temp\\ipykernel_5464\\3482997723.py:33: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  score = np.dot(x, y)/(np.linalg.norm(x)*np.linalg.norm(y))\n"
     ]
    }
   ],
   "source": [
    "lang = \"python\"\n",
    "\n",
    "corpus_tf_dict = load_pickle_file(f\"pkl/{lang}/corpus_tf_dict.pkl\")\n",
    "qid_dataframe = load_qid_dataframe(f\"{lang}\")\n",
    "\n",
    "result_dict = dict()\n",
    "for qid1, qid2 in list(qid_dataframe[[\"qid1\", \"qid2\"]].to_records(index=False)):\n",
    "    result_dict[(qid1, qid2)] = compute_cosine_similarity(corpus_tf_dict[qid1][\"title\"],\n",
    "                                                          corpus_tf_dict[qid2][\"title\"])\n",
    "\n",
    "\n",
    "result_filename = pathlib.Path(\"result/Q4.txt\")\n",
    "if result_filename.exists(): os.remove(result_filename)\n",
    "\n",
    "with open(result_filename, \"a\") as fp:\n",
    "    fp.write(\"qid1\\tqid2\\tscore\\n\")\n",
    "    for (qid1, qid2), score in result_dict.items():\n",
    "        fp.write(f\"{qid1}\\t{qid2}\\t{score}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04b6f63",
   "metadata": {},
   "source": [
    "## Question 2 (30 pts)\n",
    "\n",
    "Compute the TF-IDF score of each word in document_tf_dict and store it in the document_word_tfidf_dict.\n",
    "\n",
    "For the total number of documents N, as our LinkSO dataset is scraped from the StackOverflow website, it is a small sample of the entire pool of posts, and the exact number of posts is constantly changing (see real-time statistics here for all topics). For the sake of this assignment, we could set the total number of posts to a constant, for example, N = 10 ** 6, as an approximation.\n",
    "\n",
    "Notice the example provided as docstring is used to help you understand the input and output data structures. You are not expected to reproduce the numbers exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc1acbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_document_tfidf(document_tf_dict, \n",
    "                           corpus_df_dict):\n",
    "    \"\"\" Input: document_tf_dict: a dict of word and its term frequency in document\n",
    "               {\"i\": 1, \"love\": 1, \"python\": 1}\n",
    "               corpus_df_dict: a dict of word and its document frequencey in the entire corpus\n",
    "               {\"i\": 2, \"you\": 1, \"we\": 3, \"love\": 1, \"like\": 1, \"hate\": 2, \"python\": 5, \"c++\": 3}\n",
    "        Output: document_word_tfidf_dict: a dict of word and its TF-IDF score in the document\n",
    "               {'i': 13.592366256649782, 'love': 14.103192380416024, 'python': 12.803907396283263}\n",
    "    \"\"\"\n",
    "\n",
    "    document_word_tfidf_dict = dict()\n",
    "    #############################################START HERE#############################################\n",
    "    # Question 2 (30 pts)\n",
    "    # copy dictionary keys from tf_dict to tfidf_dict\n",
    "    document_word_tfidf_dict = dict.fromkeys(document_tf_dict)\n",
    "    \n",
    "    # Corpus length\n",
    "    # N = len(corpus_tf_dict)\n",
    "    N = 10 ** 6 # updated N as advised by Professor\n",
    "    \n",
    "    for key, value in document_word_tfidf_dict.items():\n",
    "        # caluclate tf = count of t in d / no. of words in d\n",
    "        tf = document_tf_dict[key] / sum(document_tf_dict.values())\n",
    "        \n",
    "        # calculate idf = log10(corpus length/(document frequency + 1))\n",
    "        # handles words in document but not in corpus\n",
    "        # using try and except\n",
    "        \n",
    "        # there might be some words in document that might not be in \n",
    "        # the corpus. we can mitigate with using try and except\n",
    "        \n",
    "        # use math.log10(N/(corpus_df_dict[key]+1)) when word in corpus\n",
    "        # if not, use the simplified form i.e. math.log10(N)\n",
    "        \n",
    "        # log base 10 used as all the formulations in the slide and\n",
    "        # other sources from the internet used log base 10\n",
    "        # also since N = 10^6, it is better to used log base 10\n",
    "        \n",
    "        try:\n",
    "            idf = math.log10(N/(corpus_df_dict[key]+1))\n",
    "        except:\n",
    "            idf = math.log10(N)\n",
    "            \n",
    "        # calculate tf-idf\n",
    "        document_word_tfidf_dict[key] = tf * idf\n",
    "        \n",
    "\n",
    "\n",
    "    ##############################################END HERE##############################################\n",
    "    \n",
    "    return document_word_tfidf_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0c042a",
   "metadata": {},
   "source": [
    "Test your compute_document_tfidf implementation on the title component of the Java corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec840636",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lang = \"java\"\n",
    "\n",
    "corpus_tf_dict = load_pickle_file(f\"pkl/{lang}/corpus_tf_dict.pkl\")\n",
    "corpus_df_dict = load_pickle_file(f\"pkl/{lang}/corpus_df_dict.pkl\")\n",
    "qid_dataframe = load_qid_dataframe(f\"{lang}\")\n",
    "\n",
    "result_dict = dict()\n",
    "for qid1 in qid_dataframe.qid1.tolist():\n",
    "    result_dict[qid1] = compute_document_tfidf(corpus_tf_dict[qid1][\"title\"],\n",
    "                                               corpus_df_dict)\n",
    "\n",
    "result_filename = pathlib.Path(\"result/Q5.txt\")\n",
    "if result_filename.exists(): os.remove(result_filename)\n",
    "\n",
    "with open(result_filename, \"a\") as fp:\n",
    "    fp.write(\"qid1\\ttoken\\ttfidf\\n\")\n",
    "    for qid1, d in result_dict.items():\n",
    "        for token, score in d.items():\n",
    "            fp.write(f\"{qid1}\\t{token}\\t{score}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cdff9f",
   "metadata": {},
   "source": [
    "## Question 3 (40 pts)\n",
    "\n",
    "Compute the BM25 score between query_tf_dict and candidate_tf_dict. N = 10 ** 6 following Question 2.\n",
    "\n",
    "Notice the example provided as docstring is used to help you understand the input and output data structures. You are not expected to reproduce the numbers exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "940405a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_document_bm25(query_tf_dict, \n",
    "                          candidate_tf_dict, \n",
    "                          corpus_df_dict,\n",
    "                          candidate_length,\n",
    "                          avgdl):\n",
    "    \"\"\" Input: query_tf_dict: a dict of word and its term frequency in query document\n",
    "               {\"i\": 1, \"love\": 1, \"python\": 1}     \n",
    "               candidate_tf_dict:a dict of word and its term frequency in candidate document\n",
    "               {\"i\": 1, \"like\": 1, \"c++\": 1}\n",
    "               corpus_df_dict: a dict of word and its document frequencey in the entire corpus\n",
    "               {\"i\": 2, \"you\": 1, \"we\": 3, \"love\": 1, \"like\": 1, \"hate\": 2, \"python\": 5, \"c++\": 3}\n",
    "               candidate_length: number of words in candidate document\n",
    "               3\n",
    "               avgdl: average document length in the entire corpus\n",
    "               4\n",
    "       Output: score: BM25 score between query and candidate\n",
    "               15.816571644101565\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # hyperparameters for BM25 algorithm\n",
    "    k1, b = 3, 0.75\n",
    "\n",
    "    score = 0\n",
    "    #############################################START HERE#############################################\n",
    "    # Question 3 (40 pts)\n",
    "    # N = len(corpus_tf_dict)\n",
    "    N = 10 ** 6\n",
    "    \n",
    "    # calculate idf for each word in corpus\n",
    "    # store the result in a dict named idf\n",
    "    idf = {}\n",
    "    \n",
    "    for term, freq in corpus_df_dict.items():\n",
    "        # calculate idf\n",
    "        idf[term] = math.log10(1 + (N - freq + 0.5) / (freq + 0.5))\n",
    "    \n",
    "    for term in query_tf_dict:\n",
    "        if term not in candidate_tf_dict:\n",
    "            continue\n",
    "        frequency = candidate_tf_dict[term]\n",
    "        \n",
    "        # similar to q2 try and except however in this we check something different\n",
    "        # if term not in candidate document, the frequency will be 0\n",
    "        # making the numerator 0\n",
    "        try:\n",
    "            numerator = idf[term] * frequency * (k1 + 1)\n",
    "            denominator = frequency + k1 * (1 - b + b * candidate_length / avgdl)\n",
    "        except:\n",
    "            numerator = 0\n",
    "            denominator = frequency + k1 * (1 - b + b * candidate_length / avgdl)\n",
    "        \n",
    "        score += (numerator / denominator)\n",
    "    \n",
    "    \n",
    "    ##############################################END HERE##############################################\n",
    "\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be4f67a",
   "metadata": {},
   "source": [
    "Test your compute_document_bm25 implementation on the title component of the JavaScript corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd673c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 174015/174015 [00:00<00:00, 351844.14it/s]\n"
     ]
    }
   ],
   "source": [
    "lang = \"javascript\"\n",
    "\n",
    "corpus_tf_dict = load_pickle_file(f\"pkl/{lang}/corpus_tf_dict.pkl\")\n",
    "corpus_df_dict = load_pickle_file(f\"pkl/{lang}/corpus_df_dict.pkl\")\n",
    "corpus_dl_dict = load_pickle_file(f\"pkl/{lang}/corpus_dl_dict.pkl\")\n",
    "\n",
    "qid_dataframe = load_qid_dataframe(f\"{lang}\")\n",
    "\n",
    "corpus_dataframe = load_corpus(lang=lang, verbose=True)\n",
    "avgdl = corpus_dataframe[\"title\"].apply(lambda x: len(split_text(x))).sum() / len(corpus_dataframe)\n",
    "\n",
    "result_dict = dict()\n",
    "for qid1, qid2 in list(qid_dataframe[[\"qid1\", \"qid2\"]].to_records(index=False)):\n",
    "    result_dict[(qid1, qid2)] = compute_document_bm25(corpus_tf_dict[qid1][\"title\"],\n",
    "                                                      corpus_tf_dict[qid2][\"title\"],\n",
    "                                                      corpus_df_dict,\n",
    "                                                      corpus_dl_dict[qid2][\"title\"],\n",
    "                                                      avgdl)\n",
    "\n",
    "\n",
    "result_filename = pathlib.Path(\"result/Q6.txt\")\n",
    "if result_filename.exists(): os.remove(result_filename)\n",
    "\n",
    "with open(result_filename, \"a\") as fp:\n",
    "    fp.write(\"qid1\\tqid2\\tscore\\n\")\n",
    "    for (qid1, qid2), score in result_dict.items():\n",
    "        fp.write(f\"{qid1}\\t{qid2}\\t{score}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9ea18b",
   "metadata": {},
   "source": [
    "## Running Your Ranking Algorithms\n",
    "\n",
    "The function run_retrieval_algorithm puts your implementations (compute_cosine_similarity, compute_document_tfidf, and compute_document_bm25) together and apply them to the entire dataset. Even though the code has been provided, it is recommended to read it to get a sense of how the retrieval pipeline works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9322a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = pathlib.Path(\"cs589/assignment1/dataset/\")\n",
    "\n",
    "def run_retrieval_algorithm(lang, algo, component, qid1s=None):\n",
    "    corpus_tf_dict = load_pickle_file(f\"pkl/{lang}/corpus_tf_dict.pkl\")\n",
    "    corpus_dl_dict = load_pickle_file(f\"pkl/{lang}/corpus_dl_dict.pkl\")\n",
    "    corpus_df_dict = load_pickle_file(f\"pkl/{lang}/corpus_df_dict.pkl\")\n",
    "\n",
    "    corpus_dataframe = load_corpus(lang=lang, verbose=False)\n",
    "    available_ids = corpus_dataframe.qid.unique()\n",
    "    avgdl = corpus_dataframe[component].apply(lambda x: len(split_text(x))).sum() / len(corpus_dataframe)\n",
    "\n",
    "    qid1s = qid1s if qid1s != None else load_qids(lang=lang)\n",
    "    qid1_dataframe = load_qid_dataframe(lang=lang)\n",
    "    \n",
    "    result_folder = pathlib.Path(\"result/\")\n",
    "    if not result_folder.exists(): result_folder.mkdir()\n",
    "\n",
    "    result_filename = pathlib.Path(f\"result/{lang}_{algo}_{component}.txt\")\n",
    "\n",
    "    # remove existing result file\n",
    "    if result_filename.exists():\n",
    "        os.remove(result_filename)\n",
    "\n",
    "    # write header\n",
    "    with open(result_filename, \"a\") as fp:\n",
    "        fp.write(\"qid1\\tqid2\\tscore\\tlabel\\n\")\n",
    "    \n",
    "    for qid1 in tqdm(qid1s):\n",
    "        if qid1 not in available_ids: continue\n",
    "\n",
    "        cond1 = qid1_dataframe.qid1 == qid1\n",
    "        cond2 = qid1_dataframe.label == 1\n",
    "\n",
    "        qid2s = qid1_dataframe[cond1].qid2.tolist()\n",
    "        qid2s_linked = qid1_dataframe[cond1 & cond2].qid2.tolist()\n",
    "\n",
    "        qid1_tf_dict = corpus_tf_dict[qid1][\"title\"]\n",
    "        query_result = dict()\n",
    "\n",
    "        # only for BM25\n",
    "        max_bm25 = -1\n",
    "        for qid2 in qid2s:\n",
    "            if qid2 not in available_ids: continue\n",
    "\n",
    "            qid2_tf_dict = corpus_tf_dict[qid2][component]\n",
    "\n",
    "            # tfidf\n",
    "            if algo == \"tfidf\":\n",
    "                score = compute_cosine_similarity(compute_document_tfidf(qid1_tf_dict, corpus_df_dict),\n",
    "                                                  compute_document_tfidf(qid2_tf_dict, corpus_df_dict))\n",
    "            \n",
    "            # bm25\n",
    "            if algo == \"bm25\":\n",
    "                candidate_length = corpus_dl_dict[qid2][component]\n",
    "                score = compute_document_bm25(qid1_tf_dict, \n",
    "                                              qid2_tf_dict, \n",
    "                                              corpus_df_dict,\n",
    "                                              candidate_length,\n",
    "                                              avgdl)\n",
    "                \n",
    "                max_bm25 = max(score, max_bm25)\n",
    "            \n",
    "            query_result[qid2] = score\n",
    "        \n",
    "        # adjust BM25 score\n",
    "        if (algo == \"bm25\") and (max_bm25 != 0):\n",
    "            query_result = {qid: score / max_bm25 for qid, score in query_result.items()}\n",
    "        \n",
    "        qid2s_sorted = sorted(query_result, key=query_result.get, reverse=True)\n",
    "\n",
    "        with open(result_filename, \"a\") as fp:\n",
    "            for qid2 in qid2s_sorted:\n",
    "                label = 1 if qid2 in qid2s_linked else 0\n",
    "                score = query_result[qid2]\n",
    "                \n",
    "                fp.write(f\"{qid1}\\t{qid2}\\t{score}\\t{label}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d08aaa6",
   "metadata": {},
   "source": [
    "Run the retrieval algorithms and save the ranking results for each language and each retrieval algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f020d2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running bm25 on python's title...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [30:50<00:00,  1.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running bm25 on python's question...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [30:33<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running bm25 on python's answer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [30:30<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running tfidf on python's title...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|███████████████████████████████████████████████████████████████████████▋       | 907/1000 [03:20<00:21,  4.28it/s]C:\\Users\\alexr\\AppData\\Local\\Temp\\ipykernel_5464\\3482997723.py:33: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  score = np.dot(x, y)/(np.linalg.norm(x)*np.linalg.norm(y))\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [03:42<00:00,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running tfidf on python's question...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:07<00:00,  4.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running tfidf on python's answer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                                | 4/1000 [00:00<03:40,  4.53it/s]C:\\Users\\alexr\\AppData\\Local\\Temp\\ipykernel_5464\\3482997723.py:33: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  score = np.dot(x, y)/(np.linalg.norm(x)*np.linalg.norm(y))\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [03:48<00:00,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running bm25 on java's title...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [36:10<00:00,  2.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running bm25 on java's question...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [36:05<00:00,  2.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running bm25 on java's answer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [36:11<00:00,  2.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running tfidf on java's title...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▍                                                                             | 30/1000 [00:08<04:18,  3.76it/s]C:\\Users\\alexr\\AppData\\Local\\Temp\\ipykernel_5464\\3482997723.py:33: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  score = np.dot(x, y)/(np.linalg.norm(x)*np.linalg.norm(y))\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:29<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running tfidf on java's question...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:38<00:00,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running tfidf on java's answer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▉                                                                               | 11/1000 [00:02<04:25,  3.73it/s]C:\\Users\\alexr\\AppData\\Local\\Temp\\ipykernel_5464\\3482997723.py:33: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  score = np.dot(x, y)/(np.linalg.norm(x)*np.linalg.norm(y))\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:37<00:00,  3.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running bm25 on javascript's title...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [33:35<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running bm25 on javascript's question...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [33:43<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running bm25 on javascript's answer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [33:33<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running tfidf on javascript's title...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:55<00:00,  3.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running tfidf on javascript's question...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:53<00:00,  3.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running tfidf on javascript's answer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▌                                                                              | 19/1000 [00:05<04:44,  3.45it/s]C:\\Users\\alexr\\AppData\\Local\\Temp\\ipykernel_5464\\3482997723.py:33: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  score = np.dot(x, y)/(np.linalg.norm(x)*np.linalg.norm(y))\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:54<00:00,  3.39it/s]\n"
     ]
    }
   ],
   "source": [
    "langs = [\"python\", \"java\", \"javascript\"]\n",
    "algos = [\"bm25\", \"tfidf\"]\n",
    "components = [\"title\", \"question\", \"answer\"]\n",
    "\n",
    "for lang, algo, component in itertools.product(langs, algos, components):\n",
    "    print(f\"Running {algo} on {lang}'s {component}...\")\n",
    "    run_retrieval_algorithm(lang, algo, component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346ca99b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
